{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commits to GitHub\n",
    "# Save and add a message before committing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master f734331] Finished Ch. 2 in NN&DL\n",
      " 2 files changed, 108 insertions(+), 48 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/asianzhang/machine-learning-tone-generation.git\n",
      "   3ae6b40..f734331  master -> master\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd /Users/asianzhang/Documents/GitHub/machine-learning-tone-generation\n",
    "git add .\n",
    "git commit --allow-empty-message -m \"Finished Ch. 2 in NN&DL\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "---\n",
    "# Chapter 1\n",
    "___\n",
    "## Sigmoid neurons\n",
    "---\n",
    "### Sigmoid neurons simulating perceptrons, part I\n",
    "Given that $z=w\\cdot x+b$, only $\\mbox{sgn}(z)$ affects the output of the perceptron. If $w$ and $b$ are multiplied by $c>0$, then $z=(cw)\\cdot x+ (cb)=c(w\\cdot x+b$). Since the multiplication of a number by a positive constant does not change the $ \\mbox{sgn}()$ of the number, the multiplication by $c$ will not affect the output of the perceptron.\n",
    "### Sigmoid neurons simulating perceptrons, part II \n",
    "As established in part I, multiplication of $w$ and $b$ by a constant $c>0$ will cause $z$ to be multiplied by the constant $c$. So, as $c\\to\\infty$, $|z|\\to\\infty$, and $\\lim\\limits_{z\\to-\\infty}\\sigma(z)=0$ and $\\lim\\limits_{z\\to\\infty}\\sigma(z)=1$. Since the output is either 0 or 1, this behavior models a perceptron. However, if $w\\cdot x+b = z = 0$, then $\\lim\\limits_{c\\to\\infty}cz=0$, and $\\sigma(z=0)=0.5$, which is not possible with a perceptron.\n",
    "\n",
    "\n",
    "## A simple network to classify handwritten digits\n",
    "---\n",
    "### Designing an output layer for bitwise representation of prediction\n",
    "$$ \n",
    "Weights = \\left[ {\\begin{array}{cccccccccc} \n",
    "            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 99 & 99\\\\\n",
    "            0 & 0 & 0 & 0 & 99 & 99 & 99 & 99 & 0 & 0\\\\\n",
    "            0 & 0 & 99 & 99 & 0 & 0 & 99 & 99 & 0 & 0\\\\\n",
    "            0 & 99 & 0 & 99 & 0 & 99 & 0 & 99 & 0 & 99\\\\\n",
    "            \\end{array} } \\right],\\ \n",
    "Biases = \\left[ {\\begin{array}{c}\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "## Learning with gradient descent\n",
    "---\n",
    "### Prove the following assertion:\n",
    "Assertion: The choice of $\\Delta v$ which minimizes $\\nabla C \\cdot \\Delta v  \\approx \\Delta C$ is $\\Delta v = -\\eta \\nabla C$ (where $\\eta = \\epsilon / \\|\\nabla C\\|$ is determined by the size constraint $\\|\\Delta v\\| = \\epsilon$, for some small fixed $\\epsilon>0$).\n",
    "\n",
    "According to the Cauchy-Schwarz inequality, $|\\nabla C \\cdot \\Delta v| \\leq \\|\\nabla C \\|\\\n",
    "\\|\\Delta v\\|$. Since there is a constraint $\\|\\Delta v\\|=\\epsilon$, $|\\nabla C \\cdot \\Delta v| \\leq \\epsilon\\|\\nabla C \\|$. Since $\\nabla C$ and $\\epsilon$ are constant, $\\epsilon\\|\\nabla C \\|$ is also constant. Also, since the absolute value is used in the inequality, the minimum for $\\nabla C \\cdot \\Delta v$ will be negative, leading to $\\nabla C\\cdot\\Delta v \\geq -\\epsilon\\|\\nabla C\\|$. Therefore, $\\mbox{min}(\\nabla C\\cdot\\Delta v) = \\epsilon\\|\\nabla C\\|$. $\\Delta v = -\\eta\\nabla C$ satisfies this equation, and is thus the minimum of $\\nabla C \\cdot\\Delta v$. \n",
    "\n",
    "($\\nabla C \\cdot\\Delta v=\\nabla C\\cdot(-\\eta\\nabla C)=-\\eta(\\nabla C\\cdot\\nabla C)=-\\eta\\|\\nabla C\\|^2=-\\frac{\\epsilon}{\\|\\nabla C\\|}\\|\\nabla C\\|^2=-\\epsilon\\|\\nabla C\\|$)\n",
    "### Gradient descent when $C$ is a function of just one variable\n",
    "In this case, $C$ is one-half of the squared residual between the output of a function and the expected value of the function (for confimation, see equation 6). Minimizing C leads to a smaller residual, optimizing the the function to output the desired value with the given input. The geometric interpretation is similar, with a \"ball\" ($C$) going down towards the lowest point of a hill where error is minimized.\n",
    "### Naming one advantage and one disadvantage of minibatch size of 1 (online learning) compared to a size of 20 in stochastic gradient descent\n",
    "Advantage: Less storage, i.e. no need to store $\\nabla C$ during a minibatch  \n",
    "Disadvantage: Learning may be erratic, ex. outliers will have a very high cost, leading to training in a potentially wrong direction\n",
    "\n",
    "\n",
    "## Implementing our network to classify digits\n",
    "---\n",
    "### Verifying the use of matrices with component form\n",
    "Given $a' = \\sigma(wa+b)$:  \n",
    "Since $w$ and $a$ are matrices, using the definition of matrix multiplication,\n",
    "$a_j = \\sigma((\\sum_k w_{jk} a_k) + b_j)$, which is consistent with the rule for computing the output of a sigmoid neuron.\n",
    "### Creating a network with just two layers\n",
    "Max classification accuracy: 91.88%\n",
    "# Chapter 2\n",
    "---\n",
    "## Proof of the four fundamental equations\n",
    "---\n",
    "### Proving equations BP3 and BP4\n",
    "BP3: $\\frac{\\partial C}{\\partial b^l_j}=\\delta^l_j$  \n",
    "Proof: $\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_j}=\\delta^1_j(1)=\\delta^1_j$\n",
    "\n",
    "BP4: $\\frac{\\partial C}{\\partial w^l_{jk}}=a^{l-1}_k\\delta^l_j$  \n",
    "Proof: $\\frac{\\partial C}{\\partial w^l_{jk}}=\\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}}=\\delta^l_ja^{l-1}_k=a^{l-1}_k\\delta^l_j$\n",
    "## The backpropagation algorithm\n",
    "---\n",
    "### Backpropagation with a single modified neuron\n",
    "Change BP2 so that $\\delta^l=((w^{l+1})^T\\delta^{l+1})\\odot f'(z^l)$ instead of $((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l)$\n",
    "### Backpropagation with linear neurons\n",
    "Replace all $\\sigma'(z^l)$ in the backpropagation algorithm to $1$ (i.e. remove all appearances of $\\sigma'(z^l)$).\n",
    "# Chapter 3\n",
    "---\n",
    "## The cross-entropy cost function\n",
    "---\n",
    "### Verifying the derivative of the sigmoid function\n",
    "$\\sigma(z)=\\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}$  \n",
    "$\\sigma'(z)=-(1+e^{-z})^{-2}(-e^{-z})=\\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}}=\\frac{1}{1+e^{-z}}\\frac{(1+e^{-z})-(1)}{1+e^{-z}}=\\frac{1}{1+e^{-z}}(\\frac{1+e^{-z}}{1+e^{-z}}-\\frac{(1)}{1+e^{-z}})=\\sigma(z)(1-\\sigma(z))$\n",
    "### Mixing up $y$ and $a$ in the cross entropy function\n",
    "In $-[a \\ln y + (1-a) \\ln (1-y)]$, if $y$ (the expected value for the neuron) is $0$ or $1$, there is a $\\ln 0$, which is undefined. Since $a$ is never exactly $0$ or $1$, in $-[y \\ln a  + (1-y) \\ln (1-a)]$, there will never be $\\ln 0$.\n",
    "### Verifying the use of cross-entropy for cost, $y\\neq 0$ or $1$\n",
    "$a=\\sigma(z), C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right]$  \n",
    "\n",
    "$\\frac{\\partial C}{\\partial a}=-\\frac{1}{n}\\sum_x \\left[\\frac{y}{a}-\\frac{1-y}{1-a}\\right]$\n",
    "\n",
    "When $\\sigma(z)=a=y$, $\\frac{\\partial C}{\\partial a}$ is 0, meaning that the cross-engtropy is minimized at $\\sigma(z)=a=y$ for all $y$.\n",
    "### Showing that sigmoid output activations won't always sum to $1$\n",
    "Consider a network with an output layer of one neuron. Since the output is solely within the range of the sigmoid function $(0,1)$, the output activation will never sum to $1$. In fact, even with a network with a multi-neuron output layer, having the sum of the activations equal $1$ is impossible due to the continuity of the sigmoid function.\n",
    "### Monotonicity of softmax\n",
    "$a^L_j=\\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}$  \n",
    "$\\frac{\\partial a^L_j}{\\partial z^L_{k=j}}=\\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}$  \n",
    "$\\frac{\\partial a^L_j}{\\partial z^L_{k\\neq j}}=\\frac{-e^{z^L_j}e^{z^L_k}}{\\left(\\sum_k e^{z^L_k}\\right)^2}$  \n",
    "Since $e^x>0$ for all real $x$, $\\frac{\\partial a^L_j}{\\partial z^L_{k}}$ is positive if $j=k$ and negative if $j\\neq k$.\n",
    "### Non-locality of softmax\n",
    "Since in the denominator of the softmax equation all the weighted inputs are summed over, any particular output activation $a^L_j$ depends on all the weighted inputs.\n",
    "## Overfitting and regularization\n",
    "---\n",
    "### Dangers of \"over-regularization\"\n",
    "If arbitrarily large rotations of training images are used, then one digit might look like another. This is most notable with 6 and 9, which are only different from each other by a 180Â° rotation.\n",
    "## Weight initialization\n",
    "---\n",
    "### Verifying the standard deviation of $z$ with new initialization method\n",
    "$\\sigma^2_w=\\frac{1}{n_{in}}$  \n",
    "$\\sigma^2_{neuron}=n_{in=1}\\sigma^2_w+\\sigma^2_b=\\frac{n_{in}}{2}\\frac{1}{n_{in}}+1=\\frac{3}{2}$  \n",
    "$\\sigma_{neuron}=\\sqrt{\\frac{3}{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# network.py in NN&DL\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dactivation = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dactivation = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, output=\"tuple\", quiet=False):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(1, epochs+1):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if(not(quiet)):\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                        j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print (\"Epoch {0} complete\".format(j))\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # JZ: The following basically changes the weights and biases by the mean of the nablas\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                      for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        # x is the input, y is the desired output\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = w @ activation + b\n",
    "            zs.append(z)\n",
    "            activation = self.activation(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * self.dactivation(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = delta @ activations[-2].T\n",
    "        # JZ: The meat of backprop!\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.dactivation(z)\n",
    "            delta = (self.weights[-l+1].T @ delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = delta @ activations[-l-1].T\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training networks\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n",
      "Training finished. Final classification accuracy: 9.8%\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-9b665775b6ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpercent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# print(\"Eta: {0}, Accuracy: {1}%\".format(eta, percent*100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: {1}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "training_data, validation_data, test_data = list(training_data), list(validation_data), list(test_data)\n",
    "print(\"Start training networks\")\n",
    "# Tests diferent etas for classification accuracy\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "def dReLU(x):\n",
    "    return 0.5 + 0.5*np.sign(x)\n",
    "results = [Network(784, 30, 10).SGD(training_data, 30, 10, 3.0, test_data=test_data, output=\"percent\", quiet=True)\n",
    "           for i in np.arange(15)]\n",
    "for percent in results:\n",
    "    # print(\"Eta: {0}, Accuracy: {1}%\".format(eta, percent*100))\n",
    "    print(\"Accuracy: {0}%\".format(percent*100))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 0.9163\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f675128>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 380
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = np.asarray(results)\n",
    "plt.scatter(results[:,0], results[:,1])\n",
    "\n",
    "print(\"Max accuracy:\", np.amax(results[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
