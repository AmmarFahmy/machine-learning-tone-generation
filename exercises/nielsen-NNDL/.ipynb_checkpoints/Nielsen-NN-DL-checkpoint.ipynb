{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commits to GitHub\n",
    "# Save and add a message before committing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 23bcdac] Organized files\n",
      " 3 files changed, 484 insertions(+), 20 deletions(-)\n",
      " create mode 100644 exercises/nielsen-NNDL/.ipynb_checkpoints/Nielsen-NN-DL-checkpoint.ipynb\n",
      " rename exercises/{ => nielsen-NNDL}/Nielsen-NN-DL.ipynb (98%)\n",
      " rename exercises/{ => nielsen-NNDL}/mnist.pkl.gz (100%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/asianzhang/machine-learning-tone-generation.git\n",
      "   f734331..23bcdac  master -> master\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd /Users/asianzhang/Documents/GitHub/machine-learning-tone-generation\n",
    "git add .\n",
    "git commit --allow-empty-message -m \"Organized files\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "---\n",
    "# Chapter 1\n",
    "___\n",
    "## Sigmoid neurons\n",
    "---\n",
    "### Sigmoid neurons simulating perceptrons, part I\n",
    "Given that $z=w\\cdot x+b$, only $\\mbox{sgn}(z)$ affects the output of the perceptron. If $w$ and $b$ are multiplied by $c>0$, then $z=(cw)\\cdot x+ (cb)=c(w\\cdot x+b$). Since the multiplication of a number by a positive constant does not change the $ \\mbox{sgn}()$ of the number, the multiplication by $c$ will not affect the output of the perceptron.\n",
    "### Sigmoid neurons simulating perceptrons, part II \n",
    "As established in part I, multiplication of $w$ and $b$ by a constant $c>0$ will cause $z$ to be multiplied by the constant $c$. So, as $c\\to\\infty$, $|z|\\to\\infty$, and $\\lim\\limits_{z\\to-\\infty}\\sigma(z)=0$ and $\\lim\\limits_{z\\to\\infty}\\sigma(z)=1$. Since the output is either 0 or 1, this behavior models a perceptron. However, if $w\\cdot x+b = z = 0$, then $\\lim\\limits_{c\\to\\infty}cz=0$, and $\\sigma(z=0)=0.5$, which is not possible with a perceptron.\n",
    "\n",
    "\n",
    "## A simple network to classify handwritten digits\n",
    "---\n",
    "### Designing an output layer for bitwise representation of prediction\n",
    "$$ \n",
    "Weights = \\left[ {\\begin{array}{cccccccccc} \n",
    "            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 99 & 99\\\\\n",
    "            0 & 0 & 0 & 0 & 99 & 99 & 99 & 99 & 0 & 0\\\\\n",
    "            0 & 0 & 99 & 99 & 0 & 0 & 99 & 99 & 0 & 0\\\\\n",
    "            0 & 99 & 0 & 99 & 0 & 99 & 0 & 99 & 0 & 99\\\\\n",
    "            \\end{array} } \\right],\\ \n",
    "Biases = \\left[ {\\begin{array}{c}\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            0\\\\\n",
    "            \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "## Learning with gradient descent\n",
    "---\n",
    "### Prove the following assertion:\n",
    "Assertion: The choice of $\\Delta v$ which minimizes $\\nabla C \\cdot \\Delta v  \\approx \\Delta C$ is $\\Delta v = -\\eta \\nabla C$ (where $\\eta = \\epsilon / \\|\\nabla C\\|$ is determined by the size constraint $\\|\\Delta v\\| = \\epsilon$, for some small fixed $\\epsilon>0$).\n",
    "\n",
    "According to the Cauchy-Schwarz inequality, $|\\nabla C \\cdot \\Delta v| \\leq \\|\\nabla C \\|\\\n",
    "\\|\\Delta v\\|$. Since there is a constraint $\\|\\Delta v\\|=\\epsilon$, $|\\nabla C \\cdot \\Delta v| \\leq \\epsilon\\|\\nabla C \\|$. Since $\\nabla C$ and $\\epsilon$ are constant, $\\epsilon\\|\\nabla C \\|$ is also constant. Also, since the absolute value is used in the inequality, the minimum for $\\nabla C \\cdot \\Delta v$ will be negative, leading to $\\nabla C\\cdot\\Delta v \\geq -\\epsilon\\|\\nabla C\\|$. Therefore, $\\mbox{min}(\\nabla C\\cdot\\Delta v) = \\epsilon\\|\\nabla C\\|$. $\\Delta v = -\\eta\\nabla C$ satisfies this equation, and is thus the minimum of $\\nabla C \\cdot\\Delta v$. \n",
    "\n",
    "($\\nabla C \\cdot\\Delta v=\\nabla C\\cdot(-\\eta\\nabla C)=-\\eta(\\nabla C\\cdot\\nabla C)=-\\eta\\|\\nabla C\\|^2=-\\frac{\\epsilon}{\\|\\nabla C\\|}\\|\\nabla C\\|^2=-\\epsilon\\|\\nabla C\\|$)\n",
    "### Gradient descent when $C$ is a function of just one variable\n",
    "In this case, $C$ is one-half of the squared residual between the output of a function and the expected value of the function (for confimation, see equation 6). Minimizing C leads to a smaller residual, optimizing the the function to output the desired value with the given input. The geometric interpretation is similar, with a \"ball\" ($C$) going down towards the lowest point of a hill where error is minimized.\n",
    "### Naming one advantage and one disadvantage of minibatch size of 1 (online learning) compared to a size of 20 in stochastic gradient descent\n",
    "Advantage: Less storage, i.e. no need to store $\\nabla C$ during a minibatch  \n",
    "Disadvantage: Learning may be erratic, ex. outliers will have a very high cost, leading to training in a potentially wrong direction\n",
    "\n",
    "\n",
    "## Implementing our network to classify digits\n",
    "---\n",
    "### Verifying the use of matrices with component form\n",
    "Given $a' = \\sigma(wa+b)$:  \n",
    "Since $w$ and $a$ are matrices, using the definition of matrix multiplication,\n",
    "$a_j = \\sigma((\\sum_k w_{jk} a_k) + b_j)$, which is consistent with the rule for computing the output of a sigmoid neuron.\n",
    "### Creating a network with just two layers\n",
    "Max classification accuracy: 91.88%\n",
    "# Chapter 2\n",
    "---\n",
    "## Proof of the four fundamental equations\n",
    "---\n",
    "### Proving equations BP3 and BP4\n",
    "BP3: $\\frac{\\partial C}{\\partial b^l_j}=\\delta^l_j$  \n",
    "Proof: $\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial b^l_j}=\\delta^1_j(1)=\\delta^1_j$\n",
    "\n",
    "BP4: $\\frac{\\partial C}{\\partial w^l_{jk}}=a^{l-1}_k\\delta^l_j$  \n",
    "Proof: $\\frac{\\partial C}{\\partial w^l_{jk}}=\\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}}=\\delta^l_ja^{l-1}_k=a^{l-1}_k\\delta^l_j$\n",
    "## The backpropagation algorithm\n",
    "---\n",
    "### Backpropagation with a single modified neuron\n",
    "Change BP2 so that $\\delta^l=((w^{l+1})^T\\delta^{l+1})\\odot f'(z^l)$ instead of $((w^{l+1})^T\\delta^{l+1})\\odot \\sigma'(z^l)$\n",
    "### Backpropagation with linear neurons\n",
    "Replace all $\\sigma'(z^l)$ in the backpropagation algorithm to $1$ (i.e. remove all appearances of $\\sigma'(z^l)$).\n",
    "# Chapter 3\n",
    "---\n",
    "## The cross-entropy cost function\n",
    "---\n",
    "### Verifying the derivative of the sigmoid function\n",
    "$\\sigma(z)=\\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}$  \n",
    "$\\sigma'(z)=-(1+e^{-z})^{-2}(-e^{-z})=\\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}}=\\frac{1}{1+e^{-z}}\\frac{(1+e^{-z})-(1)}{1+e^{-z}}=\\frac{1}{1+e^{-z}}(\\frac{1+e^{-z}}{1+e^{-z}}-\\frac{(1)}{1+e^{-z}})=\\sigma(z)(1-\\sigma(z))$\n",
    "### Mixing up $y$ and $a$ in the cross entropy function\n",
    "In $-[a \\ln y + (1-a) \\ln (1-y)]$, if $y$ (the expected value for the neuron) is $0$ or $1$, there is a $\\ln 0$, which is undefined. Since $a$ is never exactly $0$ or $1$, in $-[y \\ln a  + (1-y) \\ln (1-a)]$, there will never be $\\ln 0$.\n",
    "### Verifying the use of cross-entropy for cost, $y\\neq 0$ or $1$\n",
    "$a=\\sigma(z), C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right]$  \n",
    "\n",
    "$\\frac{\\partial C}{\\partial a}=-\\frac{1}{n}\\sum_x \\left[\\frac{y}{a}-\\frac{1-y}{1-a}\\right]$\n",
    "\n",
    "When $\\sigma(z)=a=y$, $\\frac{\\partial C}{\\partial a}$ is 0, meaning that the cross-engtropy is minimized at $\\sigma(z)=a=y$ for all $y$.\n",
    "### Showing that sigmoid output activations won't always sum to $1$\n",
    "Consider a network with an output layer of one neuron. Since the output is solely within the range of the sigmoid function $(0,1)$, the output activation will never sum to $1$. In fact, even with a network with a multi-neuron output layer, having the sum of the activations equal $1$ is impossible due to the continuity of the sigmoid function.\n",
    "### Monotonicity of softmax\n",
    "$a^L_j=\\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}$  \n",
    "$\\frac{\\partial a^L_j}{\\partial z^L_{k=j}}=\\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}$  \n",
    "$\\frac{\\partial a^L_j}{\\partial z^L_{k\\neq j}}=\\frac{-e^{z^L_j}e^{z^L_k}}{\\left(\\sum_k e^{z^L_k}\\right)^2}$  \n",
    "Since $e^x>0$ for all real $x$, $\\frac{\\partial a^L_j}{\\partial z^L_{k}}$ is positive if $j=k$ and negative if $j\\neq k$.\n",
    "### Non-locality of softmax\n",
    "Since in the denominator of the softmax equation all the weighted inputs are summed over, any particular output activation $a^L_j$ depends on all the weighted inputs.\n",
    "## Overfitting and regularization\n",
    "---\n",
    "### Dangers of \"over-regularization\"\n",
    "If arbitrarily large rotations of training images are used, then one digit might look like another. This is most notable with 6 and 9, which are only different from each other by a 180Â° rotation.\n",
    "## Weight initialization\n",
    "---\n",
    "### Verifying the standard deviation of $z$ with new initialization method\n",
    "$\\sigma^2_w=\\frac{1}{n_{in}}$  \n",
    "$\\sigma^2_{neuron}=n_{in=1}\\sigma^2_w+\\sigma^2_b=\\frac{n_{in}}{2}\\frac{1}{n_{in}}+1=\\frac{3}{2}$  \n",
    "$\\sigma_{neuron}=\\sqrt{\\frac{3}{2}}$\n",
    "## How to choose a network's hyper-parameters\n",
    "---\n",
    "### Obstacles to using gradient descent for $\\lambda$ and $\\eta$\n",
    "For both $\\lambda$ and $\\eta$, gradient descent would require a method that simulates the training of a network. Since a network starts with random parameters, the function that gradient descent would be performed on needs to encompass all possible starting positions, which is computationally impossible.\n",
    "### Using $\\mu>1$ or $\\mu<0$ in the momentum technique\n",
    "If $\\mu>1$, the velocity would grow exponentially, especially towards the end of learning when the gradient descent is small. If $\\mu<0$, gradient descent would oscillate between negative and positive and defeat the purpose of building momentum, as the momentum would change constantly.\n",
    "## Unfinished exercises\n",
    "--- \n",
    "[Proving the relationship between $\\sigma$ and $\\tanh$](http://neuralnetworksanddeeplearning.com/chap3.html#exercise_905893)  \n",
    "Also, look at regularization techniques (ex. dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "---\n",
    "## Chapter 2\n",
    "---\n",
    "### Alternate presentation of the equations of backpropagation\n",
    "###### BP1\n",
    "Since $\\sum'(z^L)$ is populated only on the diagonals, $\\delta^L_j = (\\sum'(z^L)_{jj}=\\sigma'(z^L)_j)\\nabla_aC_j=(\\nabla_aC\\odot\\sigma'(z^L))_j$\n",
    "###### BP2\n",
    "Using similar logic as to BP1 above, $\\delta^l=\\sum'(z^l)(w^{l+1})^T\\delta^{l+1}=\\sum'(z^l)((w^{l+1})^T\\delta^{l+1})=((w^{l+1})^T\\delta^{l+1})\\odot\\sigma'(z^l)$\n",
    "###### Combining BP1 and BP2\n",
    "$\\delta^l = \\Sigma'(z^l) (w^{l+1})^T \\ldots \\Sigma'(z^{L-1}) (w^L)^T \n",
    "    \\Sigma'(z^L) \\nabla_a C$ can be obtained by chaining together $\\delta^l$ with $\\delta^l+1$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# modification of network.py to implement fully matrix-based\n",
    "# approach to backpropagation over a mini-batch\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return .5 * (1 + np.tanh(.5 * z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class SelfNetwork2(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dadz = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dadz = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dadz = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, \n",
    "            output=\"tuple\", quiet=False, cost='cross-entropy', lmbda=5.0, \n",
    "            regularization=\"L2\", schedule=None, momentum=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        if(schedule==None or test_data==None):\n",
    "            for j in range(1, epochs+1):\n",
    "                np.random.shuffle(training_data)\n",
    "                mini_batches = [\n",
    "                    training_data[k:k+mini_batch_size]\n",
    "                    for k in range(0, n, mini_batch_size)]\n",
    "                for mini_batch in mini_batches:\n",
    "                    self.update_mini_batch(mini_batch, eta)\n",
    "                if(not(quiet)):\n",
    "                    if test_data:\n",
    "                        print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                            j, self.evaluate(test_data), n_test))\n",
    "                    else:\n",
    "                        print (\"Epoch {0} complete\".format(j))\n",
    "        else:\n",
    "            for j in range(1, 8):\n",
    "                val_accs = [0, []]\n",
    "                for i in range(0, schedule):\n",
    "                    np.random.shuffle(training_data)\n",
    "                    mini_batches = [\n",
    "                        training_data[k:k+mini_batch_size]\n",
    "                        for k in range(0, n, mini_batch_size)]\n",
    "                    for mini_batch in mini_batches:\n",
    "                        self.update_mini_batch(mini_batch, eta, \n",
    "                                               cost, lmbda, n, regularization)\n",
    "                    val_accs[1].append(self.evaluate(test_data))\n",
    "                while True:\n",
    "                    np.random.shuffle(training_data)\n",
    "                    mini_batches = [\n",
    "                        training_data[k:k+mini_batch_size]\n",
    "                        for k in range(0, n, mini_batch_size)]\n",
    "                    for mini_batch in mini_batches:\n",
    "                        self.update_mini_batch(mini_batch, eta, \n",
    "                                               cost, lmbda, n, regularization)\n",
    "                    acc = self.evaluate(test_data)\n",
    "                    if(val_accs[1][val_accs[0]]>=acc):\n",
    "                        eta /= 2\n",
    "                        break\n",
    "                    else:\n",
    "                        val_accs[1][val_accs[0]] = acc\n",
    "                        val_accs[0] += 1\n",
    "                        if(val_accs[0]==schedule):\n",
    "                            val_accs[0]=0\n",
    "\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, cost, lmbda, n, regularization):\n",
    "        nabla_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        nabla_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "        mini_batch = np.array(mini_batch)\n",
    "        mini_batch_size = mini_batch.size/2.0\n",
    "        images = np.concatenate(mini_batch[:,0], axis=1)\n",
    "        outputs = np.concatenate(mini_batch[:,1], axis=1)\n",
    "        zs = []\n",
    "        activations = [images]\n",
    "        activation = images\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            zs.append(w @ activations[-1] + b)\n",
    "            activations.append(self.activation(zs[-1]))\n",
    "        if(cost==\"quadratic\"):\n",
    "            delta = self.dCda(activations[-1],outputs)*self.dadz(zs[-1])\n",
    "        if(cost==\"cross-entropy\"):\n",
    "            delta = activations[-1] - outputs\n",
    "        nabla_b[-1] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "        nabla_w[-1] = (delta @ activations[-2].T)*eta/mini_batch_size\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = (self.weights[-l+1].T @ delta)*self.dadz(zs[-l])\n",
    "            nabla_b[-l] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "            nabla_w[-l] = (delta @ activations[-l-1].T)*eta/mini_batch_size\n",
    "        # The following basically changes the weights and biases by the mean of the nablas\n",
    "        if(regularization==\"L2\"):\n",
    "            self.weights = [(1-eta*lmbda/n)*w-nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        elif(regularization==\"L1\"):\n",
    "            self.weights = ([w-eta*lmbda/n*np.sign(w)-nw \n",
    "                            for w, nw in zip(self.weights, nabla_w)])\n",
    "        else:\n",
    "            self.weights = [w-nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = ([(b-nb.reshape(nb.shape[0],1))\n",
    "                       for b, nb in zip(self.biases, nabla_b)])\n",
    "    \n",
    "    def dCda(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        #print(self.feedforward(test_data[0][0]).shape)\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# modification of network.py to implement fully matrix-based\n",
    "# approach to backpropagation over a mini-batch\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return .5 * (1 + np.tanh(.5 * z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class SelfNetwork2(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dadz = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dadz = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dadz = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, \n",
    "            output=\"tuple\", quiet=False, cost='cross-entropy', lmbda=5.0, \n",
    "            regularization=\"L2\", schedule=None, momentum=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        if(bool(schedule==None or test_data==None) == bool(momentum==None)):\n",
    "            for j in range(1, epochs+1):\n",
    "                np.random.shuffle(training_data)\n",
    "                mini_batches = [\n",
    "                    training_data[k:k+mini_batch_size]\n",
    "                    for k in range(0, n, mini_batch_size)]\n",
    "                for mini_batch in mini_batches:\n",
    "                    self.update_mini_batch(mini_batch, eta)\n",
    "                if(not(quiet)):\n",
    "                    if test_data:\n",
    "                        print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                            j, self.evaluate(test_data), n_test))\n",
    "                    else:\n",
    "                        print (\"Epoch {0} complete\".format(j))\n",
    "        elif(not(schedule==None or test_data==None)):\n",
    "            for j in range(1, 8):\n",
    "                val_accs = [0, []]\n",
    "                for i in range(0, schedule):\n",
    "                    np.random.shuffle(training_data)\n",
    "                    mini_batches = [\n",
    "                        training_data[k:k+mini_batch_size]\n",
    "                        for k in range(0, n, mini_batch_size)]\n",
    "                    for mini_batch in mini_batches:\n",
    "                        self.update_mini_batch(mini_batch, eta, \n",
    "                                               cost, lmbda, n, regularization)\n",
    "                    val_accs[1].append(self.evaluate(test_data))\n",
    "                while True:\n",
    "                    np.random.shuffle(training_data)\n",
    "                    mini_batches = [\n",
    "                        training_data[k:k+mini_batch_size]\n",
    "                        for k in range(0, n, mini_batch_size)]\n",
    "                    for mini_batch in mini_batches:\n",
    "                        self.update_mini_batch(mini_batch, eta, \n",
    "                                               cost, lmbda, n, regularization)\n",
    "                    acc = self.evaluate(test_data)\n",
    "                    if(val_accs[1][val_accs[0]]>=acc):\n",
    "                        eta /= 2\n",
    "                        break\n",
    "                    else:\n",
    "                        val_accs[1][val_accs[0]] = acc\n",
    "                        val_accs[0] += 1\n",
    "                        if(val_accs[0]==schedule):\n",
    "                            val_accs[0]=0\n",
    "        elif(momentum!=None):\n",
    "            \n",
    "\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, cost, lmbda, n, \n",
    "                          regularization, velocity=None):\n",
    "        nabla_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        nabla_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "        mini_batch = np.array(mini_batch)\n",
    "        mini_batch_size = mini_batch.size/2.0\n",
    "        images = np.concatenate(mini_batch[:,0], axis=1)\n",
    "        outputs = np.concatenate(mini_batch[:,1], axis=1)\n",
    "        zs = []\n",
    "        activations = [images]\n",
    "        activation = images\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            zs.append(w @ activations[-1] + b)\n",
    "            activations.append(self.activation(zs[-1]))\n",
    "        if(cost==\"quadratic\"):\n",
    "            delta = self.dCda(activations[-1],outputs)*self.dadz(zs[-1])\n",
    "        if(cost==\"cross-entropy\"):\n",
    "            delta = activations[-1] - outputs\n",
    "        nabla_b[-1] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "        nabla_w[-1] = (delta @ activations[-2].T)*eta/mini_batch_size\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = (self.weights[-l+1].T @ delta)*self.dadz(zs[-l])\n",
    "            nabla_b[-l] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "            nabla_w[-l] = (delta @ activations[-l-1].T)*eta/mini_batch_size\n",
    "        # The following basically changes the weights and biases by the mean of the nablas\n",
    "        if(regularization==\"L2\"):\n",
    "            self.weights = [(1-eta*lmbda/n)*w-nw for w, nw in zip(self.weights, nabla_w)]\n",
    "            self.biases = ([(b-nb.reshape(nb.shape[0],1))\n",
    "                           for b, nb in zip(self.biases, nabla_b)])\n",
    "        elif(regularization==\"L1\"):\n",
    "            self.weights = ([w-eta*lmbda/n*np.sign(w)-nw \n",
    "                            for w, nw in zip(self.weights, nabla_w)])\n",
    "            self.biases = ([(b-nb.reshape(nb.shape[0],1))\n",
    "                           for b, nb in zip(self.biases, nabla_b)])\n",
    "        elif(regulariztion==\"momentum\"):\n",
    "            velocity[0] = velocity\n",
    "        else:\n",
    "            self.weights = [w-nw for w, nw in zip(self.weights, nabla_w)]\n",
    "            self.biases = ([(b-nb.reshape(nb.shape[0],1))\n",
    "                           for b, nb in zip(self.biases, nabla_b)])\n",
    "        return velocity\n",
    "    \n",
    "    def dCda(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        #print(self.feedforward(test_data[0][0]).shape)\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training networks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 77.14999999999999%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 90.97%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 93.5%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 94.93%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.11%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.69%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.03%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.96000000000001%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.38%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.52%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.22%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.32%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.64%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.92%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.50999999999999%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.39%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.98%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.7%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.31%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.97%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.19%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.07%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.92%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.05%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.22%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.48%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.1%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.49%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.78999999999999%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.16%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.55%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.67%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.93%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.17%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.61%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.3%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.78999999999999%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.22%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 94.89%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.55%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.19999999999999%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.86%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.14%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.1%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.72%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.06%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.02000000000001%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.72%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.25%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.0%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.74000000000001%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.24000000000001%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.49%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.82000000000001%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.96000000000001%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.69%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.34%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.62%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 94.76%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.81%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.11%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.97%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 94.97%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.07%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.12%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.98%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.71%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.17999999999999%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.37%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.1%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.16%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.75%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.75%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.21%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.75%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.21%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.93%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.47%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.04%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.54%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.57%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.48%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.44%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 94.92%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.88%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.87%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.6%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 94.93%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.33%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.48%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.35%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.8%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.22%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.62%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.38%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 94.83%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 95.26%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training finished. Final classification accuracy: 96.07%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Tests diferent etas for classification accuracy\\ndef ReLU(x):\\n    return np.maximum(x, 0)\\ndef dReLU(x):\\n    return 0.5 + 0.5*np.sign(x)\\nresults = [Network(784, 30, 10).SGD(training_data, 30, 10, 3.0, test_data=test_data, output=\"percent\", quiet=False)\\n           for i in np.arange(15)]\\nfor percent in results:\\n    # print(\"Eta: {0}, Accuracy: {1}%\".format(eta, percent*100))\\n    print(\"Accuracy: {0}%\".format(percent*100))\\nprint(\"Done\")'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "training_data, validation_data, test_data = list(training_data), list(validation_data), list(test_data)\n",
    "print(\"Start training networks\")\n",
    "# %timeit Network(784, 100, 10).SGD(training_data, 10, 10, 3.0, test_data=test_data, output=\"percent\", quiet=True)\n",
    "# %timeit OptNetwork(784, 100, 10).SGD(training_data, 10, 10, 3.0, test_data=test_data, output=\"percent\", quiet=True, cost=\"quadratic\")\n",
    "resultsl2 = [(SelfNetwork2(784, 100, 10).SGD(training_data, 30, i, 4.0, \n",
    "                                           regularization=None, test_data=test_data, \n",
    "                                           schedule=10, output=\"percent\"), i) \n",
    "           for i in range(1, 100)]\n",
    "\"\"\"# Tests diferent etas for classification accuracy\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "def dReLU(x):\n",
    "    return 0.5 + 0.5*np.sign(x)\n",
    "results = [Network(784, 30, 10).SGD(training_data, 30, 10, 3.0, test_data=test_data, output=\"percent\", quiet=False)\n",
    "           for i in np.arange(15)]\n",
    "for percent in results:\n",
    "    # print(\"Eta: {0}, Accuracy: {1}%\".format(eta, percent*100))\n",
    "    print(\"Accuracy: {0}%\".format(percent*100))\n",
    "print(\"Done\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy: 0.9661\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bff1b70>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 380
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsl2 = np.asarray(resultsl2)\n",
    "plt.scatter(resultsl2[:,1], resultsl2[:,0])\n",
    "\n",
    "print(\"Max accuracy:\", np.amax(resultsl2[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representi mng the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929 Âµs Â± 36.3 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n",
      "854 Âµs Â± 36.9 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "zeros = np.zeros([1000,1000,1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# modification of network.py to implement fully matrix-based\n",
    "# approach to backpropagation over a mini-batch\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class OptNetwork(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dadz = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dadz = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dadz = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, output=\"tuple\", quiet=False, cost='cross-entropy'):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(1, epochs+1):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, cost)\n",
    "            if(not(quiet)):\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                        j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print (\"Epoch {0} complete\".format(j))\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, cost):\n",
    "        nabla_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
    "        nabla_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
    "        mini_batch = np.array(mini_batch)\n",
    "        mini_batch_size = mini_batch.size/2.0\n",
    "        images = np.concatenate(mini_batch[:,0], axis=1)\n",
    "        outputs = np.concatenate(mini_batch[:,1], axis=1)\n",
    "        zs = []\n",
    "        activations = [images]\n",
    "        activation = images\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            zs.append(w @ activations[-1] + b)\n",
    "            activations.append(self.activation(zs[-1]))\n",
    "        if(cost==\"quadratic\"):\n",
    "            delta = self.dCda(activations[-1],outputs)*self.dadz(zs[-1])\n",
    "        if(cost==\"cross-entropy\"):\n",
    "            delta = activations[-1] - outputs\n",
    "        nabla_b[-1] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "        nabla_w[-1] = (delta @ activations[-2].T)*eta/mini_batch_size\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = (self.weights[-l+1].T @ delta)*self.dadz(zs[-l])\n",
    "            nabla_b[-l] = np.sum(delta, axis=1)*eta/mini_batch_size\n",
    "            nabla_w[-l] = (delta @ activations[-l-1].T)*eta/mini_batch_size\n",
    "        # The following basically changes the weights and biases by the mean of the nablas\n",
    "        self.weights = [w-n for w, n in zip(self.weights, nabla_w)]\n",
    "        self.biases = [(b-n.reshape(n.shape[0],1)) for b, n in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        # Not used since OptNetwork's backprop is all in update_mini_batch\n",
    "        # x is the input, y is the desired output\n",
    "        # sets shape for nabla_b and nabla_w\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward lists\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        # actual feedforward loop, saving the z matrices\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = w @ activation + b\n",
    "            zs.append(z)\n",
    "            activation = self.activation(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        # initializes lists for backprop\n",
    "        delta = self.dCda(activations[-1], y) * self.dadz(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = delta @ activations[-2].T\n",
    "        # The meat of backprop!\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.dadz(z)\n",
    "            delta = (self.weights[-l+1].T @ delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = delta @ activations[-l-1].T\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def dCda(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        #print(self.feedforward(test_data[0][0]).shape)\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# network.py in NN&DL\n",
    "\n",
    "import numpy as np\n",
    "import types\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        if(callable(args[-1]) and callable(args[-2])):\n",
    "            self.activation = args[-2]\n",
    "            self.dactivation = args[-1]\n",
    "            args = args[:-2]\n",
    "        else:\n",
    "            self.activation = sigmoid\n",
    "            self.dactivation = sigmoid_prime\n",
    "        if(isinstance(args[0], (list,))):\n",
    "            sizes = args[0]\n",
    "        else:\n",
    "            sizes = args\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        # Create a Network object net with 2 neurons in the first layer,\n",
    "        # 3 neurons in the second layer, and 1 neuron in the final\n",
    "        # layer, do\n",
    "        # net = Network([2, 3, 1])\n",
    "    \n",
    "    def setActivationFunction(self, func, dfunc):\n",
    "        self.activation = func\n",
    "        self.dactivation = dfunc\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            print(\"a\", a.shape)\n",
    "            print(\"w\", w.shape)\n",
    "            print(\"b\", b.shape)\n",
    "            print()\n",
    "            a = self.activation(w @ a + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, output=\"tuple\", quiet=False):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(1, epochs+1):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if(not(quiet)):\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                        j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print (\"Epoch {0} complete\".format(j))\n",
    "        if test_data:\n",
    "            if(output==\"tuple\"):\n",
    "                out = (self.evaluate(test_data), n_test)\n",
    "                print(\"Training finished. Final classification accuracy: {0}/{1}\".format(out[0], out[1]))\n",
    "            if(output==\"percent\"):\n",
    "                out = self.evaluate(test_data)/n_test\n",
    "                print(\"Training finished. Final classification accuracy: {0}%\".format(out*100))\n",
    "            return out\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # JZ: The following basically changes the weights and biases by the mean of the nablas\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                      for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        # x is the input, y is the desired output\n",
    "        # sets shape for nabla_b and nabla_w\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward lists\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        # actual feedforward loop, saving the z matrices\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = w @ activation + b\n",
    "            zs.append(z)\n",
    "            activation = self.activation(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        # initializes lists for backprop\n",
    "        delta = self.cost_derivative(activations[-1], y) * self.dactivation(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = delta @ activations[-2].T\n",
    "        # The meat of backprop!\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.dactivation(z)\n",
    "            delta = (self.weights[-l+1].T @ delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = delta @ activations[-l-1].T\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        print(self.feedforward(test_data[0][0]).shape)\n",
    "        print(self.weights[-2].shape)\n",
    "        return sum(int(x==y) for (x, y) in test_results)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
